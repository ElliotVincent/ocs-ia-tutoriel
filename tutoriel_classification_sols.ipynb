{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Tutoriel : Classification des sols par Deep Learning\n",
    "\n",
    "Ce tutoriel présente les bases de la classification d'images aériennes pour l'analyse de l'occupation des sols. Nous allons utiliser des méthodes de deep learning pour identifier automatiquement différents types de surfaces (cultures, forêts, surfaces artificielles, etc.) à partir d'images aériennes.\n",
    "\n",
    "## Objectif\n",
    "Construire et entraîner un réseau de neurones simple capable de classifier chaque pixel d'une image aérienne selon le type de sol qu'il représente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-intro",
   "metadata": {},
   "source": [
    "## 1. Importation des bibliothèques\n",
    "\n",
    "Nous commençons par importer les bibliothèques nécessaires :\n",
    "- **matplotlib** : pour la visualisation des images et des graphiques\n",
    "- **rasterio** : pour la lecture des images géospatiales\n",
    "- **torch** : le framework de deep learning que nous utiliserons\n",
    "- **numpy** : pour les calculs numériques\n",
    "- **sklearn** : pour les métriques d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import glob\n",
    "import warnings\n",
    "import torch \n",
    "import numpy as np \n",
    "import copy\n",
    "import sklearn.metrics\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zones-intro",
   "metadata": {},
   "source": [
    "## 2. Sélection des zones géographiques\n",
    "\n",
    "Le jeu de données contient des images de 10 zones géographiques différentes en France. Pour entraîner notre modèle de manière robuste, nous divisons ces zones en trois ensembles :\n",
    "\n",
    "- **Zone d'entraînement** : utilisée pour apprendre les paramètres du modèle\n",
    "- **Zone de validation** : utilisée pour ajuster les hyperparamètres et éviter le surapprentissage\n",
    "- **Zone de test** : utilisée pour évaluer les performances finales sur des données jamais vues\n",
    "\n",
    "**Zones disponibles** : Cairanne_84, Chateauroux_36, Chissey_71, Claveyson_26, Fessenheim_68, MaelPestivien_22, SaintCyr_69, SaintHilaire_61, SaintMartin_15, Sauvagnon_64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zones",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_entrainement = ''\n",
    "zone_validation = ''\n",
    "zone_test = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classes-intro",
   "metadata": {},
   "source": [
    "## 3. Définition des classes et des couleurs\n",
    "\n",
    "### Classes d'occupation du sol\n",
    "\n",
    "Les annotations originales comportent **19 classes** détaillées. Pour simplifier le problème dans ce tutoriel, nous les regroupons en **5 classes principales** :\n",
    "\n",
    "0. **Bâtiments** (rouge)\n",
    "1. **Sol** (jaune) - perméable, imperméable, nu, labouré, ...\n",
    "2. **Surfaces en eau** (bleu) - rivières, lacs, étangs, piscines, ...\n",
    "3. **Végétation** (vert) - forêts, haies, arbres, pelouse\n",
    "4. **Autres/Non-annoté** (noir) - ombres, ...\n",
    "\n",
    "Chaque classe est associée à une couleur RGB pour faciliter la visualisation des résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mappings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correspondance entre les 19 classes d'origine et les 5 classes simplifiées\n",
    "correspondance_19_vers_5_classes = torch.tensor([0, 1, 1, 1, 2, 3, 3, 3, 3, 3, 3, 1, 2, 4, 4, 4, 4, 4, 4])\n",
    "\n",
    "# Palette de couleurs pour les 19 classes d'origine\n",
    "couleurs_rvb_19_classes = torch.tensor([[219, 14, 154], [147, 142, 123], [248, 12, 0], [169, 113, 1], \n",
    "                                         [21, 83, 174], [25, 74, 38], [70, 228, 131], [243, 166, 13], \n",
    "                                         [102, 0, 130], [85, 255, 0], [255, 243, 13], [228, 223, 124], \n",
    "                                         [61, 230, 235], [255, 255, 255], [138, 179, 160], [107, 113, 79], \n",
    "                                         [197, 220, 66], [153, 153, 255], [0, 0, 0]])\n",
    "\n",
    "# Palette de couleurs pour les 5 classes simplifiées\n",
    "couleurs_rvb_5_classes = torch.tensor([[255, 65, 54],     # Rouge - Bâtiments\n",
    "                                        [241, 196, 15],   # Jaune - Sol\n",
    "                                        [52, 152, 219],   # Bleu - Surfaces en eau\n",
    "                                        [46, 204, 113],   # Vert - Végétation\n",
    "                                        [0, 0, 0]])       # Noir - Autres/Non-annoté"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-intro",
   "metadata": {},
   "source": [
    "## 4. Chargement des données\n",
    "\n",
    "### Structure des données\n",
    "\n",
    "Pour chaque zone géographique, nous disposons de :\n",
    "- **3 images aériennes** (format JPG) de 512×512 pixels en couleur RGB\n",
    "- **3 masques d'annotation** (format TIF) correspondants, où chaque pixel est étiqueté avec sa classe\n",
    "\n",
    "La fonction `charger_images()` charge les données d'une zone et effectue automatiquement le regroupement des 19 classes originales vers les 5 classes simplifiées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def charger_images(zone):\n",
    "    \"\"\"\n",
    "    Charge les images et annotations d'une zone géographique donnée.\n",
    "    \n",
    "    Retourne :\n",
    "    - images : tenseur des images RGB\n",
    "    - annotations_19 : annotations en 19 classes\n",
    "    - annotations_5 : annotations regroupées en 5 classes\n",
    "    \"\"\"\n",
    "    chemins_images = glob.glob(f\"sample/{zone}/IMG_*.jpg\")\n",
    "    chemins_annotations = [chemin.replace('IMG', 'MSK').replace('jpg', 'tif') for chemin in chemins_images]\n",
    "    \n",
    "    images = torch.stack([torch.tensor(rasterio.open(image).read()).float() for image in chemins_images])\n",
    "    annotations_19 = torch.stack([torch.tensor(rasterio.open(annot).read()[0]).int() for annot in chemins_annotations]) - 1\n",
    "    annotations_5 = correspondance_19_vers_5_classes[annotations_19]\n",
    "    \n",
    "    return images, annotations_19, annotations_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données pour les trois ensembles\n",
    "images_entrainement, annotations_19_entrainement, annotations_entrainement = charger_images(zone_entrainement)\n",
    "images_validation, annotations_19_validation, annotations_validation = charger_images(zone_validation)\n",
    "images_test, annotations_19_test, annotations_test = charger_images(zone_test)\n",
    "\n",
    "print(f\"Données chargées :\")\n",
    "print(f\"  - Entraînement : {images_entrainement.shape[0]} images de {images_entrainement.shape[2]}×{images_entrainement.shape[3]} pixels\")\n",
    "print(f\"  - Validation : {images_validation.shape[0]} images de {images_entrainement.shape[2]}×{images_entrainement.shape[3]} pixels\")\n",
    "print(f\"  - Test : {images_test.shape[0]} images de {images_entrainement.shape[2]}×{images_entrainement.shape[3]} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-intro",
   "metadata": {},
   "source": [
    "## 5. Visualisation des données\n",
    "\n",
    "Avant de construire notre modèle, il est essentiel de visualiser les données pour comprendre :\n",
    "- La diversité des paysages entre les différentes zones\n",
    "- La qualité des annotations\n",
    "- La distribution des classes\n",
    "\n",
    "Pour chaque image, nous affichons :\n",
    "- **Ligne 1** : les images aériennes RGB originales\n",
    "- **Ligne 2** : les masques d'annotation colorés selon les 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afficher_images(images, annotations, palette_couleurs, predictions=None):\n",
    "    \"\"\"\n",
    "    Affiche les images, leurs annotations et optionnellement les prédictions du modèle.\n",
    "    \"\"\"\n",
    "    nb_lignes = 2 if predictions is None else 3\n",
    "    \n",
    "    for k, (image, annotation) in enumerate(zip(images, annotations)):\n",
    "        img = image.permute(1, 2, 0).int()\n",
    "        annotation_coloree = palette_couleurs[annotation]\n",
    "        \n",
    "        # Affichage de l'image aérienne\n",
    "        plt.subplot(nb_lignes, 3, k + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Image {k+1}', fontsize=10)\n",
    "        \n",
    "        # Affichage de l'annotation\n",
    "        plt.subplot(nb_lignes, 3, 3 + k + 1)\n",
    "        plt.imshow(annotation_coloree)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Annotation {k+1}', fontsize=10)\n",
    "        \n",
    "        # Affichage de la prédiction si disponible\n",
    "        if predictions is not None:\n",
    "            prediction_coloree = palette_couleurs[predictions[k]]\n",
    "            plt.subplot(nb_lignes, 3, 6 + k + 1)\n",
    "            plt.imshow(prediction_coloree)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Prédiction {k+1}', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des données d'entraînement\n",
    "print(\"=== Zone d'entraînement ===\")\n",
    "afficher_images(images_entrainement, annotations_entrainement, couleurs_rvb_5_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-val",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des données de validation\n",
    "print(\"=== Zone de validation ===\")\n",
    "afficher_images(images_validation, annotations_validation, couleurs_rvb_5_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des données de test\n",
    "print(\"=== Zone de test ===\")\n",
    "afficher_images(images_test, annotations_test, couleurs_rvb_5_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-intro",
   "metadata": {},
   "source": [
    "## 6. Construction du modèle\n",
    "\n",
    "### Architecture du réseau de neurones\n",
    "\n",
    "Nous utilisons un modèle très simple pour ce tutoriel, composé de deux couches :\n",
    "\n",
    "1. **Couche de convolution 2D** : analyse l'image en appliquant des filtres qui apprennent à détecter des motifs caractéristiques de chaque classe (textures, couleurs, formes)\n",
    "   - Entrée : 3 canaux (Rouge, Vert, Bleu)\n",
    "   - Sortie : 4 canaux (une carte de scores pour chacune des 4 classes non-ignorées)\n",
    "   - Taille du filtre : 7×7 pixels (permet de capturer le contexte local autour de chaque pixel)\n",
    "\n",
    "2. **Fonction d'activation ReLU** : introduit de la non-linéarité pour permettre au modèle d'apprendre des relations complexes\n",
    "\n",
    "### Calcul du nombre de paramètres\n",
    "\n",
    "Le nombre de paramètres à apprendre est calculé ainsi :\n",
    "- Poids de la convolution : 3 (canaux d'entrée) × 4 (canaux de sortie) × 7 × 7 (taille du filtre) = 588\n",
    "- Biais : 4 (un par canal de sortie)\n",
    "- **Total : 592 paramètres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "modele = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 4, kernel_size=7, padding=3, dilation=1, groups=1, \n",
    "                    bias=True, padding_mode='reflect', device='cuda'),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "\n",
    "def compter_parametres(modele):\n",
    "    \"\"\"Calcule le nombre de paramètres apprenables du modèle.\"\"\"\n",
    "    return sum(p.numel() for p in modele.parameters() if p.requires_grad)\n",
    "\n",
    "nb_parametres = compter_parametres(modele)\n",
    "print(f\"Nombre de paramètres du modèle : {nb_parametres}\")\n",
    "print(f\"Vérification du calcul : 3 × 4 × 7 × 7 + 4 = {3 * 4 * 7 * 7 + 4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norm-intro",
   "metadata": {},
   "source": [
    "## 7. Normalisation des données\n",
    "\n",
    "### Pourquoi normaliser ?\n",
    "\n",
    "La normalisation est une étape cruciale en deep learning. Elle consiste à mettre toutes les images à la même échelle en :\n",
    "- Soustrayant la moyenne de chaque canal\n",
    "- Divisant par l'écart-type de chaque canal\n",
    "\n",
    "**Avantages** :\n",
    "- Accélère la convergence pendant l'entraînement\n",
    "- Rend le modèle moins sensible aux variations d'illumination\n",
    "- Améliore la stabilité numérique\n",
    "\n",
    "**Important** : nous calculons la moyenne et l'écart-type uniquement sur les ensembles d'entraînement et de validation, puis appliquons ces mêmes statistiques à l'ensemble de test pour éviter toute fuite d'information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des statistiques sur les données d'entraînement et de validation\n",
    "moyenne = torch.cat([images_entrainement, images_validation]).mean((0, 2, 3))\n",
    "ecart_type = torch.cat([images_entrainement, images_validation]).std((0, 2, 3))\n",
    "\n",
    "print(f\"Moyenne par canal RGB : {moyenne}\")\n",
    "print(f\"Écart-type par canal RGB : {ecart_type}\")\n",
    "\n",
    "# Application de la normalisation à tous les ensembles\n",
    "images_entrainement_norm = (images_entrainement - moyenne[..., None, None]) / ecart_type[..., None, None]\n",
    "images_validation_norm = (images_validation - moyenne[..., None, None]) / ecart_type[..., None, None]\n",
    "images_test_norm = (images_test - moyenne[..., None, None]) / ecart_type[..., None, None]\n",
    "\n",
    "print(\"\\nNormalisation appliquée avec succès\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-intro",
   "metadata": {},
   "source": [
    "## 8. Entraînement du modèle\n",
    "\n",
    "### Processus d'apprentissage\n",
    "\n",
    "L'entraînement consiste à ajuster les 592 paramètres du modèle pour minimiser l'erreur de classification. Le processus se déroule en plusieurs étapes :\n",
    "\n",
    "1. **Propagation avant** : le modèle prédit les classes pour chaque pixel\n",
    "2. **Calcul de la perte** : on mesure l'écart entre les prédictions et les vraies annotations\n",
    "3. **Rétropropagation** : on calcule comment ajuster les paramètres pour réduire l'erreur\n",
    "4. **Mise à jour** : les paramètres sont ajustés dans la bonne direction\n",
    "\n",
    "### Composants utilisés\n",
    "\n",
    "- **Fonction de perte** : Cross-Entropy Loss (mesure la qualité de la classification)\n",
    "- **Optimiseur** : SGD avec momentum (algorithme qui ajuste les paramètres)\n",
    "- **Scheduler** : ajuste automatiquement le taux d'apprentissage selon les performances\n",
    "\n",
    "### Métriques suivies\n",
    "\n",
    "- **Précision (accuracy)** : pourcentage de pixels correctement classés\n",
    "- **Perte (loss)** : mesure de l'erreur globale du modèle\n",
    "\n",
    "Ces métriques sont calculées à chaque époque sur l'entraînement et tous les 5 époques sur la validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfert des données vers le GPU pour accélérer les calculs\n",
    "images_entrainement_norm = images_entrainement_norm.to('cuda')\n",
    "annotations_entrainement = annotations_entrainement.to('cuda')\n",
    "images_validation_norm = images_validation_norm.to('cuda')\n",
    "annotations_validation = annotations_validation.to('cuda')\n",
    "\n",
    "# Configuration de l'entraînement\n",
    "optimiseur = torch.optim.SGD(modele.parameters(), lr=0.01, momentum=0.9)\n",
    "planificateur = torch.optim.lr_scheduler.MultiStepLR(optimiseur, milestones=[1500, 1750], gamma=0.1)\n",
    "critere = torch.nn.CrossEntropyLoss(ignore_index=4)  # Ignore la classe \"autre\"\n",
    "\n",
    "# Variables pour suivre l'évolution\n",
    "precisions_entrainement, precisions_validation = [], []\n",
    "pertes_entrainement, pertes_validation = [], []\n",
    "meilleure_precision_val = 0\n",
    "meilleur_modele = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle d'entraînement sur 100 époques\n",
    "nb_epoques = 2000\n",
    "val_frequence = 100\n",
    "\n",
    "for epoque in range(1, nb_epoques + 1):\n",
    "    # Phase d'entraînement\n",
    "    modele.train()\n",
    "    optimiseur.zero_grad()\n",
    "    \n",
    "    sortie = modele(images_entrainement_norm)\n",
    "    prediction = torch.argmax(sortie, dim=1)\n",
    "    perte = critere(sortie, annotations_entrainement)\n",
    "    precision = (prediction == annotations_entrainement).sum() / (512 * 512 * 3) * 100\n",
    "    \n",
    "    perte.backward()\n",
    "    optimiseur.step()\n",
    "    planificateur.step(precision)\n",
    "    \n",
    "    pertes_entrainement.append(float(perte.item()))\n",
    "    precisions_entrainement.append(float(precision.item()))\n",
    "    \n",
    "    # Évaluation sur la validation tous les 5 époques\n",
    "    if epoque % val_frequence == 0:\n",
    "        modele.eval()\n",
    "        with torch.no_grad():\n",
    "            sortie_val = modele(images_validation_norm)\n",
    "            prediction_val = torch.argmax(sortie_val, dim=1)\n",
    "            perte_val = float(critere(sortie_val, annotations_validation).item())\n",
    "            precision_val = float((prediction_val == annotations_validation).sum() / (512 * 512 * 3) * 100)\n",
    "            \n",
    "            pertes_validation.append(perte_val)\n",
    "            precisions_validation.append(precision_val)\n",
    "            \n",
    "            # Sauvegarde du meilleur modèle\n",
    "            if precision_val > meilleure_precision_val:\n",
    "                meilleure_precision_val = precision_val\n",
    "                meilleur_modele = copy.deepcopy(modele)\n",
    "        \n",
    "        print(f\"Époque {epoque}/{nb_epoques} - \"\n",
    "              f\"Perte train: {perte:.4f}, Précision train: {precision:.2f}% - \"\n",
    "              f\"Perte val: {perte_val:.4f}, Précision val: {precision_val:.2f}%\")\n",
    "\n",
    "print(\"\\nEntraînement terminé !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curves-intro",
   "metadata": {},
   "source": [
    "## 9. Visualisation des courbes d'apprentissage\n",
    "\n",
    "Les courbes d'apprentissage permettent de diagnostiquer la qualité de l'entraînement :\n",
    "\n",
    "### Courbe de précision\n",
    "- **Si train et validation augmentent ensemble** : le modèle apprend correctement\n",
    "- **Si train augmente mais validation stagne** : surapprentissage (le modèle mémorise les données d'entraînement)\n",
    "\n",
    "### Courbe de perte\n",
    "- **Si train et validation diminuent ensemble** : convergence normale\n",
    "- **Si train diminue mais validation augmente** : surapprentissage\n",
    "\n",
    "Une bonne généralisation se traduit par des courbes de train et validation qui évoluent de manière similaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe de précision\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, nb_epoques + 1), precisions_entrainement, label='Entraînement', linewidth=2)\n",
    "plt.plot(range(val_frequence, nb_epoques + 1, val_frequence), precisions_validation, label='Validation', linewidth=2, marker='o')\n",
    "plt.xlabel('Époque', fontsize=12)\n",
    "plt.ylabel('Précision (%)', fontsize=12)\n",
    "plt.title('Évolution de la précision', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Courbe de perte\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, nb_epoques + 1), pertes_entrainement, label='Entraînement', linewidth=2)\n",
    "plt.plot(range(val_frequence, nb_epoques + 1, val_frequence), pertes_validation, label='Validation', linewidth=2, marker='o')\n",
    "plt.xlabel('Époque', fontsize=12)\n",
    "plt.ylabel('Perte', fontsize=12)\n",
    "plt.title('Évolution de la perte', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-intro",
   "metadata": {},
   "source": [
    "## 10. Évaluation sur les données de validation\n",
    "\n",
    "### Matrice de confusion\n",
    "\n",
    "La matrice de confusion est un outil essentiel pour analyser les performances du modèle. Elle montre :\n",
    "- **Diagonale** : nombre de pixels correctement classés pour chaque classe\n",
    "- **Hors diagonale** : confusions entre classes (ex: forêts classées comme prairies)\n",
    "\n",
    "Cette analyse permet d'identifier quelles classes sont difficiles à distinguer et pourquoi le modèle se trompe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrice_de_confusion(images, annotations, split):\n",
    "    \n",
    "    # Génération des prédictions sur la validation avec le meilleur modèle\n",
    "    meilleur_modele.eval()\n",
    "    with torch.no_grad():\n",
    "        sortie = meilleur_modele(images.to('cuda'))\n",
    "        predictions = torch.argmax(sortie, dim=1)\n",
    "    \n",
    "    # Calcul de la matrice de confusion\n",
    "    matrice_confusion = sklearn.metrics.confusion_matrix(\n",
    "        annotations.cpu().flatten().numpy(), \n",
    "        predictions.cpu().flatten().numpy(), \n",
    "        labels=[0, 1, 2, 3]\n",
    "    )\n",
    "    \n",
    "    # Normalisation de la matrice de confusion par ligne (par classe réelle)\n",
    "    # Ne pas ajouter 1e-10, laisser les divisions par zéro créer des NaN\n",
    "    sommes_lignes = matrice_confusion.sum(axis=1)[:, np.newaxis]\n",
    "    matrice_confusion_norm = np.divide(\n",
    "        matrice_confusion.astype('float'), \n",
    "        sommes_lignes,\n",
    "        where=sommes_lignes != 0  # Évite le warning de division par zéro\n",
    "    )\n",
    "    # Les lignes avec somme = 0 auront des NaN\n",
    "    matrice_confusion_norm[sommes_lignes.flatten() == 0] = np.nan\n",
    "    \n",
    "    noms_classes = ['Bâti', 'Sol', 'Eau', 'Végét']\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Créer une copie pour l'affichage où NaN = 0 (pour le colormap)\n",
    "    matrice_affichage = np.copy(matrice_confusion_norm)\n",
    "    matrice_affichage[np.isnan(matrice_affichage)] = 0\n",
    "    \n",
    "    plt.imshow(matrice_affichage, interpolation='nearest', cmap='Blues')\n",
    "    plt.title(f'Matrice de confusion ({split})', fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(label='Nombre de pixels')\n",
    "    \n",
    "    # Ajout des étiquettes\n",
    "    tick_marks = np.arange(len(noms_classes))\n",
    "    plt.xticks(tick_marks, noms_classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, noms_classes)\n",
    "    \n",
    "    # Affichage des valeurs dans chaque case\n",
    "    for i in range(len(noms_classes)):\n",
    "        for j in range(len(noms_classes)):\n",
    "            if np.isnan(matrice_confusion_norm[i, j]):\n",
    "                # Case grisée pour NaN\n",
    "                plt.gca().add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, \n",
    "                                                   fill=True, color='lightgray', \n",
    "                                                   edgecolor='white', linewidth=2))\n",
    "                plt.text(j, i, 'N/A', ha=\"center\", va=\"center\", \n",
    "                        color='darkgray', fontsize=12, fontweight='bold')\n",
    "            else:\n",
    "                couleur_texte = 'white' if matrice_confusion_norm[i, j] > np.nanmax(matrice_confusion_norm) / 2 else 'black'\n",
    "                plt.text(j, i, format(matrice_confusion_norm[i, j]*100, '.1f'),\n",
    "                        ha=\"center\", va=\"center\", color=couleur_texte, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.ylabel('Vraie classe', fontsize=12)\n",
    "    plt.xlabel('Classe prédite', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return predictions\n",
    "    \n",
    "\n",
    "predictions_entrainement = matrice_de_confusion(images_entrainement_norm, annotations_entrainement, \"Entraînement\")\n",
    "predictions_validation = matrice_de_confusion(images_validation_norm, annotations_validation, \"Validation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-pred-intro",
   "metadata": {},
   "source": [
    "## 11. Visualisation des prédictions sur les données de test\n",
    "\n",
    "Nous évaluons maintenant le modèle sur l'ensemble de test, composé d'images d'une zone géographique jamais vue pendant l'entraînement. Cela permet de mesurer la capacité de **généralisation** du modèle.\n",
    "\n",
    "Pour chaque image, nous affichons :\n",
    "- **Ligne 1** : l'image aérienne originale\n",
    "- **Ligne 2** : l'annotation de référence (vérité terrain)\n",
    "- **Ligne 3** : la prédiction du modèle\n",
    "\n",
    "Comparer visuellement ces trois lignes permet d'identifier les forces et faiblesses du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération des prédictions sur le jeu d'entraînement\n",
    "meilleur_modele.eval()\n",
    "with torch.no_grad():\n",
    "    sortie_entrainement = modele(images_entrainement_norm.to('cuda'))\n",
    "    predictions_entrainement = torch.argmax(sortie_entrainement, dim=1)\n",
    "\n",
    "# Génération des prédictions sur le jeu de test\n",
    "meilleur_modele.eval()\n",
    "with torch.no_grad():\n",
    "    sortie_test = modele(images_test_norm.to('cuda'))\n",
    "    predictions_test = torch.argmax(sortie_test, dim=1)\n",
    "\n",
    "\n",
    "print(\"=== Résultats sur la zone d'entraînement ===\")\n",
    "afficher_images(images_entrainement, annotations_entrainement.cpu(), couleurs_rvb_5_classes, predictions_entrainement.cpu())\n",
    "\n",
    "print(\"=== Résultats sur la zone de test ===\")\n",
    "afficher_images(images_test, annotations_test, couleurs_rvb_5_classes, predictions_test.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a5bcac-b98e-493a-8054-5873b8349f82",
   "metadata": {},
   "source": [
    "### Matrice de confusion\n",
    "\n",
    "On peut également afficher la matrice de confusion sur le jeu de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ccad36-f5cc-4dbe-b961-0749f81e25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = matrice_de_confusion(images_test_norm, annotations_test, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-intro",
   "metadata": {},
   "source": [
    "## 12. Métriques de performance\n",
    "\n",
    "Pour quantifier objectivement les performances du modèle, nous calculons plusieurs métriques :\n",
    "\n",
    "### Précision globale (Overall Accuracy)\n",
    "Pourcentage de pixels correctement classés tous types confondus.\n",
    "\n",
    "### Précision par classe (Per-class Accuracy)\n",
    "Pourcentage de pixels correctement classés pour chaque type d'occupation du sol. Permet d'identifier les classes bien reconnues vs les classes problématiques.\n",
    "\n",
    "### Précision moyenne (Mean Accuracy)\n",
    "Moyenne des précisions par classe. Cette métrique donne le même poids à toutes les classes, contrairement à la précision globale qui favorise les classes majoritaires.\n",
    "\n",
    "**Classes** :\n",
    "- 0 : Bâtiment\n",
    "- 1 : Sol\n",
    "- 2 : Surfaces en eau\n",
    "- 3 : Végétation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64feed16-5d1d-4cea-a2dc-6a749032773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def calculer_metriques(annotations, predictions, labels=[0, 1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Calcule les métriques de performance en gérant les classes absentes.\n",
    "    \"\"\"\n",
    "    # Calcul de la matrice de confusion\n",
    "    matrice_confusion = sklearn.metrics.confusion_matrix(\n",
    "        annotations.cpu().flatten().numpy() if hasattr(annotations, 'numpy') else annotations.flatten(),\n",
    "        predictions.cpu().flatten().numpy() if hasattr(predictions, 'cpu') else predictions.flatten().numpy(),\n",
    "        labels=labels\n",
    "    )\n",
    "    \n",
    "    # Précision globale (accuracy)\n",
    "    precision_globale = np.trace(matrice_confusion) / np.sum(matrice_confusion) * 100\n",
    "    \n",
    "    # Calcul de la précision par classe (recall) en gérant les divisions par zéro\n",
    "    sommes_lignes = matrice_confusion.sum(axis=1)\n",
    "    precision_par_classe = np.divide(\n",
    "        np.diag(matrice_confusion).astype('float'),\n",
    "        sommes_lignes,\n",
    "        out=np.full_like(sommes_lignes, np.nan, dtype='float'),\n",
    "        where=sommes_lignes != 0\n",
    "    ) * 100\n",
    "    \n",
    "    # Précision moyenne (mean accuracy) sur les classes présentes\n",
    "    precision_moyenne = np.nanmean(precision_par_classe)\n",
    "    \n",
    "    return {\n",
    "        'Précision globale (%)': precision_globale,\n",
    "        'Précision moyenne (%)': precision_moyenne,\n",
    "        'Bâtiment (%)': precision_par_classe[0],\n",
    "        'Sol (%)': precision_par_classe[1],\n",
    "        'Eau (%)': precision_par_classe[2],\n",
    "        'Végétation (%)': precision_par_classe[3],\n",
    "    }\n",
    "\n",
    "# Calcul des métriques pour chaque ensemble\n",
    "metriques_train = calculer_metriques(annotations_entrainement, predictions_entrainement)\n",
    "metriques_val = calculer_metriques(annotations_validation, predictions_validation)\n",
    "metriques_test = calculer_metriques(annotations_test, predictions_test)\n",
    "\n",
    "# Création du DataFrame\n",
    "df_metriques = pd.DataFrame({\n",
    "    'Entraînement': metriques_train,\n",
    "    'Validation': metriques_val,\n",
    "    'Test': metriques_test\n",
    "}).T\n",
    "\n",
    "# Formatage du tableau\n",
    "def formater_cellule(valeur):\n",
    "    \"\"\"Formate les valeurs : affiche N/A pour NaN, sinon le pourcentage.\"\"\"\n",
    "    if pd.isna(valeur):\n",
    "        return 'N/A'\n",
    "    else:\n",
    "        return f'{valeur:.2f}'\n",
    "\n",
    "# Application du formatage\n",
    "df_formate = df_metriques.map(formater_cellule)\n",
    "\n",
    "try:\n",
    "    styled_df = df_formate.style.set_properties(**{\n",
    "        'text-align': 'center',\n",
    "        'font-size': '12pt',\n",
    "    }).set_table_styles([\n",
    "        {'selector': 'th', 'props': [('font-weight', 'bold'), ('text-align', 'center')]},\n",
    "        {'selector': 'td', 'props': [('padding', '8px')]}\n",
    "    ])\n",
    "    display(styled_df)\n",
    "except:\n",
    "    print(df_formate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conclusion et perspectives\n",
    "\n",
    "### Ce que nous avons accompli\n",
    "\n",
    "Dans ce tutoriel, nous avons construit un modèle simple de classification d'images aériennes capable d'identifier automatiquement différents types d'occupation du sol. Bien que l'architecture soit minimaliste (592 paramètres seulement), elle démontre les concepts fondamentaux du deep learning appliqué à la télédétection.\n",
    "\n",
    "### Limites du modèle actuel\n",
    "\n",
    "- Architecture très simple : une seule couche de convolution limite la capacité à capturer des motifs complexes\n",
    "- Petit jeu de données : seulement 3 images par zone\n",
    "- Contexte spatial limité : le filtre 7×7 ne capte qu'un voisinage très local\n",
    "\n",
    "### Vers le modèle CoSIA\n",
    "\n",
    "- L'architecture Swin+UperNet du modèle CoSIA comporte environ 90 million de paramètres\n",
    "- Le jeu de données d'entraînement de ce modèle représente 2500 km² répartis sur tout le territoire métropolitain avec des prises de vues allant d'avril à novembre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
