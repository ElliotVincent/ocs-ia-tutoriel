{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Tutoriel : Classification des sols par Deep Learning\n",
    "\n",
    "Ce tutoriel présente les bases de la classification d'images satellite pour l'analyse de l'occupation des sols. Nous allons utiliser des méthodes de deep learning pour identifier automatiquement différents types de surfaces (cultures, forêts, surfaces artificielles, etc.) à partir d'images aériennes.\n",
    "\n",
    "## Objectif\n",
    "Construire et entraîner un réseau de neurones simple capable de classifier chaque pixel d'une image satellite selon le type de sol qu'il représente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-intro",
   "metadata": {},
   "source": [
    "## 1. Importation des bibliothèques\n",
    "\n",
    "Nous commençons par importer les bibliothèques nécessaires :\n",
    "- **matplotlib** : pour la visualisation des images et des graphiques\n",
    "- **rasterio** : pour la lecture des images géospatiales\n",
    "- **torch** : le framework de deep learning que nous utiliserons\n",
    "- **numpy** : pour les calculs numériques\n",
    "- **sklearn** : pour les métriques d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import glob\n",
    "import warnings\n",
    "import torch \n",
    "import numpy as np \n",
    "import copy\n",
    "import sklearn.metrics\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zones-intro",
   "metadata": {},
   "source": [
    "## 2. Sélection des zones géographiques\n",
    "\n",
    "Le jeu de données contient des images de 10 zones géographiques différentes en France. Pour entraîner notre modèle de manière robuste, nous divisons ces zones en trois ensembles :\n",
    "\n",
    "- **Zone d'entraînement** : utilisée pour apprendre les paramètres du modèle\n",
    "- **Zone de validation** : utilisée pour ajuster les hyperparamètres et éviter le surapprentissage\n",
    "- **Zone de test** : utilisée pour évaluer les performances finales sur des données jamais vues\n",
    "\n",
    "**Zones disponibles** : Cairanne_84, Chateauroux_36, Chissey_71, Claveyson_26, Fessenheim_68, MaelPestivien_22, SaintCyr_69, SaintHilaire_61, SaintMartin_15, Sauvagnon_64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zones",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_entrainement = 'Sauvagnon_64'\n",
    "zone_validation = 'MaelPestivien_22'\n",
    "zone_test = 'Chateauroux_36'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classes-intro",
   "metadata": {},
   "source": [
    "## 3. Définition des classes et des couleurs\n",
    "\n",
    "### Classes d'occupation du sol\n",
    "\n",
    "Les annotations originales comportent **19 classes** détaillées. Pour simplifier le problème dans ce tutoriel, nous les regroupons en **5 classes principales** :\n",
    "\n",
    "0. **Cultures annuelles** (rouge) - terres arables, cultures céréalières\n",
    "1. **Prairies et végétation herbacée** (jaune) - prairies permanentes, pâturages\n",
    "2. **Surfaces en eau** (bleu) - rivières, lacs, étangs\n",
    "3. **Forêts et végétation ligneuse** (vert) - forêts, haies, arbres\n",
    "4. **Surfaces artificielles et autres** (noir) - bâtiments, routes, zones non classées\n",
    "\n",
    "Chaque classe est associée à une couleur RGB pour faciliter la visualisation des résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mappings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correspondance entre les 19 classes d'origine et les 5 classes simplifiées\n",
    "correspondance_19_vers_5_classes = torch.tensor([0, 1, 1, 1, 2, 3, 3, 3, 3, 3, 3, 1, 2, 4, 4, 4, 4, 4, 4])\n",
    "\n",
    "# Palette de couleurs pour les 19 classes d'origine\n",
    "couleurs_rvb_19_classes = torch.tensor([[219, 14, 154], [147, 142, 123], [248, 12, 0], [169, 113, 1], \n",
    "                                         [21, 83, 174], [25, 74, 38], [70, 228, 131], [243, 166, 13], \n",
    "                                         [102, 0, 130], [85, 255, 0], [255, 243, 13], [228, 223, 124], \n",
    "                                         [61, 230, 235], [255, 255, 255], [138, 179, 160], [107, 113, 79], \n",
    "                                         [197, 220, 66], [153, 153, 255], [0, 0, 0]])\n",
    "\n",
    "# Palette de couleurs pour les 5 classes simplifiées\n",
    "couleurs_rvb_5_classes = torch.tensor([[255, 65, 54],    # Rouge - Cultures annuelles\n",
    "                                        [241, 196, 15],   # Jaune - Prairies\n",
    "                                        [52, 152, 219],   # Bleu - Surfaces en eau\n",
    "                                        [46, 204, 113],   # Vert - Forêts\n",
    "                                        [0, 0, 0]])       # Noir - Surfaces artificielles/autres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-intro",
   "metadata": {},
   "source": [
    "## 4. Chargement des données\n",
    "\n",
    "### Structure des données\n",
    "\n",
    "Pour chaque zone géographique, nous disposons de :\n",
    "- **3 images satellites** (format JPG) de 512×512 pixels en couleur RGB\n",
    "- **3 masques d'annotation** (format TIF) correspondants, où chaque pixel est étiqueté avec sa classe\n",
    "\n",
    "La fonction `charger_images()` charge les données d'une zone et effectue automatiquement le regroupement des 19 classes originales vers les 5 classes simplifiées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def charger_images(zone):\n",
    "    \"\"\"\n",
    "    Charge les images et annotations d'une zone géographique donnée.\n",
    "    \n",
    "    Retourne :\n",
    "    - images : tenseur des images RGB\n",
    "    - annotations_19 : annotations en 19 classes\n",
    "    - annotations_5 : annotations regroupées en 5 classes\n",
    "    \"\"\"\n",
    "    chemins_images = glob.glob(f\"sample/{zone}/IMG_*.jpg\")\n",
    "    chemins_annotations = [chemin.replace('IMG', 'MSK').replace('jpg', 'tif') for chemin in chemins_images]\n",
    "    \n",
    "    images = torch.stack([torch.tensor(rasterio.open(image).read()).float() for image in chemins_images])\n",
    "    annotations_19 = torch.stack([torch.tensor(rasterio.open(annot).read()[0]).int() for annot in chemins_annotations]) - 1\n",
    "    annotations_5 = correspondance_19_vers_5_classes[annotations_19]\n",
    "    \n",
    "    return images, annotations_19, annotations_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données pour les trois ensembles\n",
    "images_entrainement, annotations_19_entrainement, annotations_entrainement = charger_images(zone_entrainement)\n",
    "images_validation, annotations_19_validation, annotations_validation = charger_images(zone_validation)\n",
    "images_test, annotations_19_test, annotations_test = charger_images(zone_test)\n",
    "\n",
    "print(f\"Données chargées :\")\n",
    "print(f\"  - Entraînement : {images_entrainement.shape[0]} images de {images_entrainement.shape[2]}×{images_entrainement.shape[3]} pixels\")\n",
    "print(f\"  - Validation : {images_validation.shape[0]} images\")\n",
    "print(f\"  - Test : {images_test.shape[0]} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-intro",
   "metadata": {},
   "source": [
    "## 5. Visualisation des données\n",
    "\n",
    "Avant de construire notre modèle, il est essentiel de visualiser les données pour comprendre :\n",
    "- La diversité des paysages entre les différentes zones\n",
    "- La qualité des annotations\n",
    "- La distribution des classes\n",
    "\n",
    "Pour chaque image, nous affichons :\n",
    "- **Ligne 1** : les images satellites RGB originales\n",
    "- **Ligne 2** : les masques d'annotation colorés selon les 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afficher_images(images, annotations, palette_couleurs, predictions=None):\n",
    "    \"\"\"\n",
    "    Affiche les images, leurs annotations et optionnellement les prédictions du modèle.\n",
    "    \"\"\"\n",
    "    nb_lignes = 2 if predictions is None else 3\n",
    "    \n",
    "    for k, (image, annotation) in enumerate(zip(images, annotations)):\n",
    "        img = image.permute(1, 2, 0).int()\n",
    "        annotation_coloree = palette_couleurs[annotation]\n",
    "        \n",
    "        # Affichage de l'image satellite\n",
    "        plt.subplot(nb_lignes, 3, k + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Image {k+1}', fontsize=10)\n",
    "        \n",
    "        # Affichage de l'annotation\n",
    "        plt.subplot(nb_lignes, 3, 3 + k + 1)\n",
    "        plt.imshow(annotation_coloree)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Annotation {k+1}', fontsize=10)\n",
    "        \n",
    "        # Affichage de la prédiction si disponible\n",
    "        if predictions is not None:\n",
    "            prediction_coloree = palette_couleurs[predictions[k]]\n",
    "            plt.subplot(nb_lignes, 3, 6 + k + 1)\n",
    "            plt.imshow(prediction_coloree)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Prédiction {k+1}', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des données d'entraînement\n",
    "print(\"=== Zone d'entraînement ===\")\n",
    "afficher_images(images_entrainement, annotations_entrainement, couleurs_rvb_5_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-val",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des données de validation\n",
    "print(\"=== Zone de validation ===\")\n",
    "afficher_images(images_validation, annotations_validation, couleurs_rvb_5_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des données de test\n",
    "print(\"=== Zone de test ===\")\n",
    "afficher_images(images_test, annotations_test, couleurs_rvb_5_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-intro",
   "metadata": {},
   "source": [
    "## 6. Construction du modèle\n",
    "\n",
    "### Architecture du réseau de neurones\n",
    "\n",
    "Nous utilisons un modèle très simple pour ce tutoriel, composé de deux couches :\n",
    "\n",
    "1. **Couche de convolution 2D** : analyse l'image en appliquant des filtres qui apprennent à détecter des motifs caractéristiques de chaque classe (textures, couleurs, formes)\n",
    "   - Entrée : 3 canaux (Rouge, Vert, Bleu)\n",
    "   - Sortie : 4 canaux (une carte de scores pour chacune des 4 classes non-ignorées)\n",
    "   - Taille du filtre : 7×7 pixels (permet de capturer le contexte local autour de chaque pixel)\n",
    "\n",
    "2. **Fonction d'activation ReLU** : introduit de la non-linéarité pour permettre au modèle d'apprendre des relations complexes\n",
    "\n",
    "### Calcul du nombre de paramètres\n",
    "\n",
    "Le nombre de paramètres à apprendre est calculé ainsi :\n",
    "- Poids de la convolution : 3 (canaux d'entrée) × 4 (canaux de sortie) × 7 × 7 (taille du filtre) = 588\n",
    "- Biais : 4 (un par canal de sortie)\n",
    "- **Total : 592 paramètres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "modele = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 4, kernel_size=7, padding=3, dilation=1, groups=1, \n",
    "                    bias=True, padding_mode='reflect', device='cuda'),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "\n",
    "def compter_parametres(modele):\n",
    "    \"\"\"Calcule le nombre de paramètres apprenables du modèle.\"\"\"\n",
    "    return sum(p.numel() for p in modele.parameters() if p.requires_grad)\n",
    "\n",
    "nb_parametres = compter_parametres(modele)\n",
    "print(f\"Nombre de paramètres du modèle : {nb_parametres}\")\n",
    "print(f\"Vérification du calcul : 3 × 4 × 7 × 7 + 4 = {3 * 4 * 7 * 7 + 4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norm-intro",
   "metadata": {},
   "source": [
    "## 7. Normalisation des données\n",
    "\n",
    "### Pourquoi normaliser ?\n",
    "\n",
    "La normalisation est une étape cruciale en deep learning. Elle consiste à mettre toutes les images à la même échelle en :\n",
    "- Soustrayant la moyenne de chaque canal\n",
    "- Divisant par l'écart-type de chaque canal\n",
    "\n",
    "**Avantages** :\n",
    "- Accélère la convergence pendant l'entraînement\n",
    "- Rend le modèle moins sensible aux variations d'illumination\n",
    "- Améliore la stabilité numérique\n",
    "\n",
    "**Important** : nous calculons la moyenne et l'écart-type uniquement sur les ensembles d'entraînement et de validation, puis appliquons ces mêmes statistiques à l'ensemble de test pour éviter toute fuite d'information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des statistiques sur les données d'entraînement et de validation\n",
    "moyenne = torch.cat([images_entrainement, images_validation]).mean((0, 2, 3))\n",
    "ecart_type = torch.cat([images_entrainement, images_validation]).std((0, 2, 3))\n",
    "\n",
    "print(f\"Moyenne par canal RGB : {moyenne}\")\n",
    "print(f\"Écart-type par canal RGB : {ecart_type}\")\n",
    "\n",
    "# Application de la normalisation à tous les ensembles\n",
    "images_entrainement_norm = (images_entrainement - moyenne[..., None, None]) / ecart_type[..., None, None]\n",
    "images_validation_norm = (images_validation - moyenne[..., None, None]) / ecart_type[..., None, None]\n",
    "images_test_norm = (images_test - moyenne[..., None, None]) / ecart_type[..., None, None]\n",
    "\n",
    "print(\"\\nNormalisation appliquée avec succès\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-intro",
   "metadata": {},
   "source": [
    "## 8. Entraînement du modèle\n",
    "\n",
    "### Processus d'apprentissage\n",
    "\n",
    "L'entraînement consiste à ajuster les 592 paramètres du modèle pour minimiser l'erreur de classification. Le processus se déroule en plusieurs étapes :\n",
    "\n",
    "1. **Propagation avant** : le modèle prédit les classes pour chaque pixel\n",
    "2. **Calcul de la perte** : on mesure l'écart entre les prédictions et les vraies annotations\n",
    "3. **Rétropropagation** : on calcule comment ajuster les paramètres pour réduire l'erreur\n",
    "4. **Mise à jour** : les paramètres sont ajustés dans la bonne direction\n",
    "\n",
    "### Composants utilisés\n",
    "\n",
    "- **Fonction de perte** : Cross-Entropy Loss (mesure la qualité de la classification)\n",
    "- **Optimiseur** : SGD avec momentum (algorithme qui ajuste les paramètres)\n",
    "- **Scheduler** : ajuste automatiquement le taux d'apprentissage selon les performances\n",
    "\n",
    "### Métriques suivies\n",
    "\n",
    "- **Précision (accuracy)** : pourcentage de pixels correctement classés\n",
    "- **Perte (loss)** : mesure de l'erreur globale du modèle\n",
    "\n",
    "Ces métriques sont calculées à chaque époque sur l'entraînement et tous les 5 époques sur la validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfert des données vers le GPU pour accélérer les calculs\n",
    "images_entrainement_norm = images_entrainement_norm.to('cuda')\n",
    "annotations_entrainement = annotations_entrainement.to('cuda')\n",
    "images_validation_norm = images_validation_norm.to('cuda')\n",
    "annotations_validation = annotations_validation.to('cuda')\n",
    "\n",
    "# Configuration de l'entraînement\n",
    "optimiseur = torch.optim.SGD(modele.parameters(), lr=0.05, momentum=0.9)\n",
    "planificateur = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiseur, \"max\")\n",
    "critere = torch.nn.CrossEntropyLoss(ignore_index=4)  # Ignore la classe \"autre\"\n",
    "\n",
    "# Variables pour suivre l'évolution\n",
    "precisions_entrainement, precisions_validation = [], []\n",
    "pertes_entrainement, pertes_validation = [], []\n",
    "meilleure_perte_val = torch.inf\n",
    "meilleur_modele = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle d'entraînement sur 100 époques\n",
    "nb_epoques = 100\n",
    "\n",
    "for epoque in range(1, nb_epoques + 1):\n",
    "    # Phase d'entraînement\n",
    "    modele.train()\n",
    "    optimiseur.zero_grad()\n",
    "    \n",
    "    sortie = modele(images_entrainement_norm)\n",
    "    prediction = torch.argmax(sortie, dim=1)\n",
    "    perte = critere(sortie, annotations_entrainement)\n",
    "    precision = (prediction == annotations_entrainement).sum() / (512 * 512 * 3) * 100\n",
    "    \n",
    "    perte.backward()\n",
    "    optimiseur.step()\n",
    "    planificateur.step(precision)\n",
    "    \n",
    "    pertes_entrainement.append(float(perte.item()))\n",
    "    precisions_entrainement.append(float(precision.item()))\n",
    "    \n",
    "    # Évaluation sur la validation tous les 5 époques\n",
    "    if epoque % 5 == 0:\n",
    "        modele.eval()\n",
    "        with torch.no_grad():\n",
    "            sortie_val = modele(images_validation_norm)\n",
    "            prediction_val = torch.argmax(sortie_val, dim=1)\n",
    "            perte_val = float(critere(sortie_val, annotations_validation).item())\n",
    "            precision_val = float((prediction_val == annotations_validation).sum() / (512 * 512 * 3) * 100)\n",
    "            \n",
    "            pertes_validation.append(perte_val)\n",
    "            precisions_validation.append(precision_val)\n",
    "            \n",
    "            # Sauvegarde du meilleur modèle\n",
    "            if perte_val < meilleure_perte_val:\n",
    "                meilleure_perte_val = perte_val\n",
    "                meilleur_modele = copy.deepcopy(modele)\n",
    "        \n",
    "        print(f\"Époque {epoque}/{nb_epoques} - \"\n",
    "              f\"Perte train: {perte:.4f}, Précision train: {precision:.2f}% - \"\n",
    "              f\"Perte val: {perte_val:.4f}, Précision val: {precision_val:.2f}%\")\n",
    "\n",
    "print(\"\\nEntraînement terminé !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curves-intro",
   "metadata": {},
   "source": [
    "## 9. Visualisation des courbes d'apprentissage\n",
    "\n",
    "Les courbes d'apprentissage permettent de diagnostiquer la qualité de l'entraînement :\n",
    "\n",
    "### Courbe de précision\n",
    "- **Si train et validation augmentent ensemble** : le modèle apprend correctement\n",
    "- **Si train augmente mais validation stagne** : surapprentissage (le modèle mémorise les données d'entraînement)\n",
    "\n",
    "### Courbe de perte\n",
    "- **Si train et validation diminuent ensemble** : convergence normale\n",
    "- **Si train diminue mais validation augmente** : surapprentissage\n",
    "\n",
    "Une bonne généralisation se traduit par des courbes de train et validation qui évoluent de manière similaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe de précision\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, nb_epoques + 1), precisions_entrainement, label='Entraînement', linewidth=2)\n",
    "plt.plot(range(5, nb_epoques + 1, 5), precisions_validation, label='Validation', linewidth=2, marker='o')\n",
    "plt.xlabel('Époque', fontsize=12)\n",
    "plt.ylabel('Précision (%)', fontsize=12)\n",
    "plt.title('Évolution de la précision', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Courbe de perte\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, nb_epoques + 1), pertes_entrainement, label='Entraînement', linewidth=2)\n",
    "plt.plot(range(5, nb_epoques + 1, 5), pertes_validation, label='Validation', linewidth=2, marker='o')\n",
    "plt.xlabel('Époque', fontsize=12)\n",
    "plt.ylabel('Perte', fontsize=12)\n",
    "plt.title('Évolution de la perte', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-intro",
   "metadata": {},
   "source": [
    "## 10. Évaluation sur les données de validation\n",
    "\n",
    "### Matrice de confusion\n",
    "\n",
    "La matrice de confusion est un outil essentiel pour analyser les performances du modèle. Elle montre :\n",
    "- **Diagonale** : nombre de pixels correctement classés pour chaque classe\n",
    "- **Hors diagonale** : confusions entre classes (ex: forêts classées comme prairies)\n",
    "\n",
    "Cette analyse permet d'identifier quelles classes sont difficiles à distinguer et pourquoi le modèle se trompe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération des prédictions sur la validation avec le meilleur modèle\n",
    "meilleur_modele.eval()\n",
    "with torch.no_grad():\n",
    "    sortie_val = meilleur_modele(images_validation_norm.to('cuda'))\n",
    "    predictions_val = torch.argmax(sortie_val, dim=1)\n",
    "\n",
    "# Calcul de la matrice de confusion\n",
    "matrice_confusion = sklearn.metrics.confusion_matrix(\n",
    "    annotations_validation.cpu().flatten().numpy(), \n",
    "    predictions_val.cpu().flatten().numpy(), \n",
    "    labels=[0, 1, 2, 3]\n",
    ")\n",
    "\n",
    "print(\"Matrice de confusion (Validation) :\")\n",
    "print(\"Lignes = vraies classes, Colonnes = classes prédites\\n\")\n",
    "print(\"        Cultures  Prairies  Eau      Forêts\")\n",
    "print(matrice_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-pred-intro",
   "metadata": {},
   "source": [
    "## 11. Visualisation des prédictions sur les données de test\n",
    "\n",
    "Nous évaluons maintenant le modèle sur l'ensemble de test, composé d'images d'une zone géographique jamais vue pendant l'entraînement. Cela permet de mesurer la capacité de **généralisation** du modèle.\n",
    "\n",
    "Pour chaque image, nous affichons :\n",
    "- **Ligne 1** : l'image satellite originale\n",
    "- **Ligne 2** : l'annotation de référence (vérité terrain)\n",
    "- **Ligne 3** : la prédiction du modèle\n",
    "\n",
    "Comparer visuellement ces trois lignes permet d'identifier les forces et faiblesses du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération des prédictions sur le test\n",
    "meilleur_modele.eval()\n",
    "with torch.no_grad():\n",
    "    sortie_test = meilleur_modele(images_test_norm.to('cuda'))\n",
    "    predictions_test = torch.argmax(sortie_test, dim=1)\n",
    "\n",
    "print(\"=== Résultats sur la zone de test ===\")\n",
    "afficher_images(images_test, annotations_test, couleurs_rvb_5_classes, predictions_test.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-intro",
   "metadata": {},
   "source": [
    "## 12. Métriques de performance\n",
    "\n",
    "Pour quantifier objectivement les performances du modèle, nous calculons plusieurs métriques :\n",
    "\n",
    "### Précision globale (Overall Accuracy)\n",
    "Pourcentage de pixels correctement classés tous types confondus.\n",
    "\n",
    "### Précision par classe (Per-class Accuracy)\n",
    "Pourcentage de pixels correctement classés pour chaque type d'occupation du sol. Permet d'identifier les classes bien reconnues vs les classes problématiques.\n",
    "\n",
    "### Précision moyenne (Mean Accuracy)\n",
    "Moyenne des précisions par classe. Cette métrique donne le même poids à toutes les classes, contrairement à la précision globale qui favorise les classes majoritaires.\n",
    "\n",
    "**Classes** :\n",
    "- 0 : Cultures annuelles\n",
    "- 1 : Prairies\n",
    "- 2 : Surfaces en eau\n",
    "- 3 : Forêts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la matrice de confusion sur le test\n",
    "matrice_confusion_test = sklearn.metrics.confusion_matrix(\n",
    "    annotations_test.flatten().numpy(), \n",
    "    predictions_test.cpu().flatten().numpy(), \n",
    "    labels=[0, 1, 2, 3]\n",
    ")\n",
    "\n",
    "# Calcul des métriques\n",
    "precision_globale = np.trace(matrice_confusion_test) / np.sum(matrice_confusion_test) * 100\n",
    "precision_par_classe = np.diag(matrice_confusion_test) / (matrice_confusion_test.sum(axis=1) + 1e-17) * 100\n",
    "precision_moyenne = np.mean(precision_par_classe)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RÉSULTATS FINAUX SUR L'ENSEMBLE DE TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPrécision globale : {precision_globale:.2f}%\")\n",
    "print(f\"Précision moyenne : {precision_moyenne:.2f}%\")\n",
    "print(f\"\\nPrécision par classe :\")\n",
    "print(f\"  - Cultures annuelles : {precision_par_classe[0]:.2f}%\")\n",
    "print(f\"  - Prairies : {precision_par_classe[1]:.2f}%\")\n",
    "print(f\"  - Surfaces en eau : {precision_par_classe[2]:.2f}%\")\n",
    "print(f\"  - Forêts : {precision_par_classe[3]:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion et perspectives\n",
    "\n",
    "### Ce que nous avons accompli\n",
    "\n",
    "Dans ce tutoriel, nous avons construit un modèle simple de classification d'images satellite capable d'identifier automatiquement différents types d'occupation du sol. Bien que l'architecture soit minimaliste (592 paramètres seulement), elle démontre les concepts fondamentaux du deep learning appliqué à la télédétection.\n",
    "\n",
    "### Limites du modèle actuel\n",
    "\n",
    "- Architecture très simple : une seule couche de convolution limite la capacité à capturer des motifs complexes\n",
    "- Petit jeu de données : seulement 3 images par zone\n",
    "- Contexte spatial limité : le filtre 7×7 ne capte qu'un voisinage très local\n",
    "\n",
    "### Pistes d'amélioration\n",
    "\n",
    "Pour obtenir de meilleures performances, on pourrait :\n",
    "1. **Utiliser une architecture plus profonde** (UNet, DeepLab, SegFormer) avec plusieurs couches de convolution\n",
    "2. **Augmenter les données** par rotation, flip, changement de luminosité\n",
    "3. **Ajouter plus de données d'entraînement** provenant de zones géographiques variées\n",
    "4. **Intégrer des informations temporelles** en utilisant des séries d'images multi-dates\n",
    "5. **Utiliser du transfer learning** en partant d'un modèle pré-entraîné sur ImageNet\n",
    "\n",
    "### Applications pratiques\n",
    "\n",
    "Les techniques présentées ici sont utilisées en production pour :\n",
    "- Le suivi de l'agriculture et des cultures\n",
    "- La surveillance de la déforestation\n",
    "- La cartographie de l'urbanisation\n",
    "- La gestion des ressources naturelles\n",
    "- L'aide à la décision en aménagement du territoire"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
